# UROP-2019
Using Sparse SimBA attack to study the nature of adversarial attacks against CNNs. Credit to @alvarorobledo, @nvsabhilash and  @bethgelab for Sparse SimBA, CAM keras and Foolbox repositories/folders.

## sparse_simba

Algorithm for generating adversarial attacks, developed by @alvarorobledo. Contains the script main_sparse_simba.py which is essentially his implementation but rewritten as a function. This is called by other main functions (in root directory) to carry out multiple experiments. 

Running this attack requires image data in the form of 2 numpy arrays in the "data" directory of sparse_simba - x_val_1000.npy (image data) and y_val_1000.npy (corresponding labels). These can be generated by downloading the ImageNet 2012 competition validation set into a directory named "val" which should be placed in the "data" subdirectory of "sparse_simba". Then, run the Jupyter Notebook "1. Preprocess ImageNet .... .ipynb". Edit the jupyter notebook to change the size of the generated numpy arrays/which sections of the imagenet validation they contain.

## CAM_keras

Developed by @nvs-abhilash, this implementation takes his code and places it in functions to be called by main functions in root directory.

## main_targeted.py

This function:
- runs a targeted attack
- plots the successful adversarial noise distribution against the CAM of the input image, target image and adversarial image
- saves the above in a directory

Targeted attacks need a "targeted_split.npy" array to work. This is an array of tuples of the form [original image index, target image index] (note that this is 0-indexed). Look through the "val" subdirectory (under sparse simba) to see the imagenet validation set and pick a pair of images that you would like to run a targeted attack on.

targeted_split.npy can be generated using "target_split_creator.py" - just update the array in the script and hit run.


## main_untargeted.py

This function:
- runs an untargeted attack on a particular image n times
- finds the average of the n successful adversarial noise distributions
- plots a comparison of the average adversarial noise distribution with the CAM's of original and adversarial images
- saves the above in a new directory

Untargeted attacks need an "untargeted_split.npy" array to work. This is just the indices of the images being attacked - pick from the same "val" folder.

## Results

Some example results and plots from running the above two main functions. Note that some results folders might be empty - this indicates that the attacks were unsuccessful (usually because the targeted attack pairing could not be acheived given the PSNR and SSIM constraints).
